{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Projects and Training\\Projects\\floor plan\\deep_layout\\train\\Living\\config_living.py:36: UserWarning: Warning: opt has not attribut ip\n",
      "  warnings.warn(\"Warning: opt has not attribut %s\" % k)\n",
      "d:\\Projects and Training\\Projects\\floor plan\\deep_layout\\train\\Living\\config_living.py:36: UserWarning: Warning: opt has not attribut stdin\n",
      "  warnings.warn(\"Warning: opt has not attribut %s\" % k)\n",
      "d:\\Projects and Training\\Projects\\floor plan\\deep_layout\\train\\Living\\config_living.py:36: UserWarning: Warning: opt has not attribut control\n",
      "  warnings.warn(\"Warning: opt has not attribut %s\" % k)\n",
      "d:\\Projects and Training\\Projects\\floor plan\\deep_layout\\train\\Living\\config_living.py:36: UserWarning: Warning: opt has not attribut hb\n",
      "  warnings.warn(\"Warning: opt has not attribut %s\" % k)\n",
      "d:\\Projects and Training\\Projects\\floor plan\\deep_layout\\train\\Living\\config_living.py:36: UserWarning: Warning: opt has not attribut Session.signature_scheme\n",
      "  warnings.warn(\"Warning: opt has not attribut %s\" % k)\n",
      "d:\\Projects and Training\\Projects\\floor plan\\deep_layout\\train\\Living\\config_living.py:36: UserWarning: Warning: opt has not attribut Session.key\n",
      "  warnings.warn(\"Warning: opt has not attribut %s\" % k)\n",
      "d:\\Projects and Training\\Projects\\floor plan\\deep_layout\\train\\Living\\config_living.py:36: UserWarning: Warning: opt has not attribut shell\n",
      "  warnings.warn(\"Warning: opt has not attribut %s\" % k)\n",
      "d:\\Projects and Training\\Projects\\floor plan\\deep_layout\\train\\Living\\config_living.py:36: UserWarning: Warning: opt has not attribut transport\n",
      "  warnings.warn(\"Warning: opt has not attribut %s\" % k)\n",
      "d:\\Projects and Training\\Projects\\floor plan\\deep_layout\\train\\Living\\config_living.py:36: UserWarning: Warning: opt has not attribut iopub\n",
      "  warnings.warn(\"Warning: opt has not attribut %s\" % k)\n",
      "d:\\Projects and Training\\Projects\\floor plan\\deep_layout\\train\\Living\\config_living.py:36: UserWarning: Warning: opt has not attribut f\n",
      "  warnings.warn(\"Warning: opt has not attribut %s\" % k)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user config:\n",
      "train_data_root: D:\\Projects and Training\\Projects\\floor plan\\dataset\\dataset\\pickle\\train\n",
      "val_data_root: D:\\Projects and Training\\Projects\\floor plan\\dataset\\dataset\\pickle\\val\n",
      "save_log_root: log\n",
      "result_file: result_living.csv\n",
      "module_name: living\n",
      "model_name: resnet34_fc1\n",
      "load_model_path: None\n",
      "load_connect_path: None\n",
      "mask_size: 9\n",
      "multi_GPU: False\n",
      "batch_size: 16\n",
      "num_workers: 2\n",
      "print_freq: 300\n",
      "max_epoch: 300\n",
      "current_epoch: 0\n",
      "save_freq: 50\n",
      "val_freq: 5\n",
      "update_lr: True\n",
      "lr_decay_freq: 30\n",
      "lr_base: 0.0001\n",
      "weight_decay: 0.0001\n",
      "parse: <bound method LivingConfig.parse of <config_living.LivingConfig object at 0x00000178DD96DA80>>\n",
      "Training start time: Oct 20 2023 13:45:49\n",
      "Building model...\n",
      "Building dataset...\n",
      "Building data loader...\n",
      "Building criterion and optimizer...\n",
      "Starting to train...\n",
      "\n",
      "Training epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "299it [04:24,  1.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 74.13238\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "599it [08:36,  1.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 48.39147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "899it [12:53,  1.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 36.89139\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1199it [17:20,  1.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 27.95836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1499it [22:04,  1.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 20.22199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1799it [26:39,  1.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 15.28788\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2099it [31:01,  2.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 12.69644\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2399it [34:07,  2.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 11.96378\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2699it [37:28,  2.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 11.29944\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2999it [40:05,  2.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 11.17175\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3299it [42:38,  2.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 11.14345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3599it [45:19,  1.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 10.95348\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3899it [47:40,  2.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 10.80465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4040it [49:00,  1.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 22.85744\n",
      "\n",
      "Training epoch: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "299it [02:44,  2.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 10.40325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "599it [06:05,  2.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 10.47281\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "899it [09:11,  2.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 10.21055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1199it [12:25,  1.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 10.12944\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1499it [16:01,  1.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 10.18387\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1799it [19:05,  2.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 10.04552\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2099it [22:28,  1.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 9.92381\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2399it [25:25,  1.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 9.49989\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2493it [26:25,  2.16it/s]"
     ]
    }
   ],
   "source": [
    "from config_living import LivingConfig\n",
    "from torch.utils.data import DataLoader\n",
    "from data import LivingDataset\n",
    "from inspect import getsource\n",
    "from torchnet import meter\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch as t\n",
    "import models\n",
    "import utils\n",
    "import fire\n",
    "import time\n",
    "import csv\n",
    "import os\n",
    "\n",
    "opt = LivingConfig()\n",
    "log = utils.log\n",
    "\n",
    "\n",
    "def train(**kwargs):\n",
    "    name = time.strftime('living_train_%Y%m%d_%H%M%S')\n",
    "    log_file = open(f\"{opt.save_log_root}/{name}.txt\", 'w')\n",
    "\n",
    "    opt.parse(kwargs, log_file)\n",
    "    start_time = time.strftime(\"%b %d %Y %H:%M:%S\")\n",
    "    log(log_file, f'Training start time: {start_time}')\n",
    "\n",
    "    # step1: configure model\n",
    "    log(log_file, 'Building model...')\n",
    "    model = models.model(\n",
    "        module_name=opt.module_name,\n",
    "        model_name=opt.model_name,\n",
    "        input_channel=3,\n",
    "        output_channel=2,\n",
    "        pretrained=False\n",
    "    )\n",
    "    input_channel = 512\n",
    "    connect = models.connect(\n",
    "        module_name=opt.module_name,\n",
    "        model_name=opt.model_name,\n",
    "        input_channel=input_channel,\n",
    "        output_channel=2,\n",
    "        reshape=True\n",
    "    )\n",
    "\n",
    "    if opt.multi_GPU:\n",
    "        model_parallel = models.ParallelModule(model=model)\n",
    "        connect_parallel = models.ParallelModule(model=connect)\n",
    "        if opt.load_model_path:\n",
    "            model_parallel.load_model(opt.load_model_path)\n",
    "        if opt.load_connect_path:\n",
    "            connect_parallel.load_model(opt.load_connect_path)\n",
    "        model = model_parallel.model\n",
    "        connect = connect_parallel.model\n",
    "    else:\n",
    "        if opt.load_model_path:\n",
    "            model.load_model(opt.load_model_path)\n",
    "        if opt.load_connect_path:\n",
    "            connect.load_model(opt.load_connect_path)\n",
    "    model.cuda()\n",
    "    connect.cuda()\n",
    "\n",
    "    # step2: data\n",
    "    log(log_file, 'Building dataset...')\n",
    "    train_data = LivingDataset(\n",
    "        data_root=opt.train_data_root, mask_size=opt.mask_size)\n",
    "    val_data = LivingDataset(\n",
    "        data_root=opt.val_data_root, mask_size=opt.mask_size)\n",
    "\n",
    "    log(log_file, 'Building data loader...')\n",
    "    train_dataloader = DataLoader(\n",
    "        train_data,\n",
    "        opt.batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=opt.num_workers\n",
    "    )\n",
    "    val_dataloader = DataLoader(\n",
    "        val_data,\n",
    "        opt.batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=opt.num_workers\n",
    "    )\n",
    "\n",
    "    # step3: criterion and optimizer\n",
    "    log(log_file, 'Building criterion and optimizer...')\n",
    "    lr = opt.lr_base\n",
    "    optimizer = t.optim.Adam(\n",
    "        list(model.parameters())+list(connect.parameters()),\n",
    "        lr=lr,\n",
    "        weight_decay=opt.weight_decay\n",
    "    )\n",
    "    current_epoch = opt.current_epoch\n",
    "    # criterion = t.nn.MSELoss()\n",
    "    criterion = t.nn.SmoothL1Loss()\n",
    "    loss_meter = meter.AverageValueMeter()\n",
    "\n",
    "    # step4: training\n",
    "    log(log_file, 'Starting to train...')\n",
    "    if current_epoch == 0 and os.path.exists(opt.result_file):\n",
    "        os.remove(opt.result_file)\n",
    "    result_file = open(opt.result_file, 'a', newline='')\n",
    "    writer = csv.writer(result_file)\n",
    "    if current_epoch == 0:\n",
    "        data_name = ['Epoch', 'Average Loss', 'Val Loss']\n",
    "        writer.writerow(data_name)\n",
    "        result_file.flush()\n",
    "\n",
    "    while current_epoch < opt.max_epoch:\n",
    "        current_epoch += 1\n",
    "        running_loss = 0.0\n",
    "        loss_meter.reset()\n",
    "        log(log_file)\n",
    "        log(log_file, f'Training epoch: {current_epoch}')\n",
    "\n",
    "        for i, (input, target) in tqdm(enumerate(train_dataloader)):\n",
    "            input = input.cuda()\n",
    "            target = target.cuda()\n",
    "            optimizer.zero_grad()\n",
    "            score_model = model(input)\n",
    "            score_connect = connect(score_model)\n",
    "            loss = criterion(score_connect, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # log info\n",
    "            running_loss += loss.item()\n",
    "            if i % opt.print_freq == opt.print_freq - 1:\n",
    "                log(log_file, f'loss {running_loss / opt.print_freq:.5f}')\n",
    "                running_loss = 0.0\n",
    "            loss_meter.add(loss.item())\n",
    "\n",
    "        if current_epoch % opt.save_freq == 0:\n",
    "            if opt.multi_GPU:\n",
    "                model_parallel.save_model(current_epoch)\n",
    "                connect_parallel.save_model(current_epoch)\n",
    "            else:\n",
    "                model.save_model(current_epoch)\n",
    "                connect.save_model(current_epoch)\n",
    "        average_loss = round(loss_meter.value()[0], 5)\n",
    "        log(log_file, f'Average Loss: {average_loss}')\n",
    "\n",
    "        # validate\n",
    "        if current_epoch % opt.val_freq == 0:\n",
    "            val_error = val(model, connect, val_dataloader, log_file)\n",
    "            results = [current_epoch, average_loss, val_error]\n",
    "            writer.writerow(results)\n",
    "            result_file.flush()\n",
    "\n",
    "        # update learning rate\n",
    "        if opt.update_lr:\n",
    "            if current_epoch % opt.lr_decay_freq == 0:\n",
    "                lr = lr * 0.5\n",
    "                for param_group in optimizer.param_groups:\n",
    "                    param_group['lr'] = lr\n",
    "                    log(log_file, f'Updating learning rate: {lr}')\n",
    "\n",
    "    end_time = time.strftime(\"%b %d %Y %H:%M:%S\")\n",
    "    log(log_file, f'Training end time: {end_time}')\n",
    "    log_file.close()\n",
    "    result_file.close()\n",
    "\n",
    "\n",
    "def val(model, connect, dataloader, file):\n",
    "    model.eval()\n",
    "    connect.eval()\n",
    "    error_meter = meter.AverageValueMeter()\n",
    "\n",
    "    for _, (input, target) in enumerate(dataloader):\n",
    "        batch_size = input.shape[0]\n",
    "        with t.no_grad():\n",
    "            input = input.cuda()\n",
    "            score_model = model(input)\n",
    "            score_connect = connect(score_model)\n",
    "\n",
    "        output = score_connect.cpu().numpy().astype(int)\n",
    "        target = target.numpy()\n",
    "        distance_error = (output[:, 0]-target[:, 0])**2 + \\\n",
    "            (output[:, 1]-target[:, 1])**2\n",
    "        for i in range(batch_size):\n",
    "            error_meter.add(distance_error[i]**0.5 * utils.pixel2length)\n",
    "\n",
    "    model.train()\n",
    "    connect.train()\n",
    "    val_error = round(error_meter.value()[0], 5)\n",
    "    log(file, f'Val Error: {val_error}')\n",
    "    return val_error\n",
    "\n",
    "\n",
    "def help():\n",
    "    print(\"\"\"\n",
    "    usage : python file.py <function> [--args=value]\n",
    "    <function> := train | help\n",
    "    example: \n",
    "            python {0} train --lr=0.01\n",
    "            python {0} help\n",
    "    avaiable args:\"\"\".format(__file__))\n",
    "\n",
    "    source = (getsource(opt.__class__))\n",
    "    print(source)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    fire.Fire(train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_science",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
