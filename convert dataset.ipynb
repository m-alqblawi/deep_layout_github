{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "59251fed-e23f-49ef-ba94-3c5ed083b933",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import zipfile\n",
    "# with zipfile.ZipFile(\"/home/u202194/tmp/dataset.zip\",\"r\") as zip_ref:\n",
    "#     zip_ref.extractall(\"/home/u202194/tmp/floor plan\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ed00e7a5-e348-44b4-a1f3-cefb239de510",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/u202194/tmp\n"
     ]
    }
   ],
   "source": [
    "# !pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2445020-46a0-4fc4-8e69-f3d712137fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = r\"D:\\Projects and Training\\Projects\\floor plan\\dataset\\dataset\\floorplan_dataset\"  # Replace with the path to your dataset\n",
    "train_dir = r\"D:\\Projects and Training\\Projects\\floor plan\\dataset\\dataset\\train\"  # Replace with the path to save the training set\n",
    "validate_dir = r\"D:\\Projects and Training\\Projects\\floor plan\\dataset\\dataset\\val\"  # Replace with the path to save the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e754130-5a76-4779-b0de-8c4c5501245c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split the dataset into 64630 training images and 16158 validation images.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "from shutil import copyfile\n",
    "\n",
    "# import torch\n",
    "# from torchvision import datasets, transforms\n",
    "\n",
    "# Define the split ratio (e.g., 80% for training, 20% for validation)\n",
    "split_ratio = 0.8\n",
    "\n",
    "# Create the output directories if they don't exist\n",
    "os.makedirs(train_dir, exist_ok=True)\n",
    "os.makedirs(validate_dir, exist_ok=True)\n",
    "\n",
    "# List all the PNG files in the dataset directory\n",
    "image_files = [f for f in os.listdir(dataset_dir) if f.endswith(\".png\")]\n",
    "\n",
    "# Shuffle the image files randomly\n",
    "random.shuffle(image_files)\n",
    "\n",
    "# Calculate the number of files for training and validation\n",
    "num_total = len(image_files)\n",
    "num_train = int(num_total * split_ratio)\n",
    "num_validate = num_total - num_train\n",
    "\n",
    "# Split the dataset\n",
    "train_files = image_files[:num_train]\n",
    "validate_files = image_files[num_train:]\n",
    "\n",
    "# Copy the training images to the train directory\n",
    "for filename in train_files:\n",
    "    source_path = os.path.join(dataset_dir, filename)\n",
    "    destination_path = os.path.join(train_dir, filename)\n",
    "    copyfile(source_path, destination_path)\n",
    "\n",
    "# Copy the validation images to the validate directory\n",
    "for filename in validate_files:\n",
    "    source_path = os.path.join(dataset_dir, filename)\n",
    "    destination_path = os.path.join(validate_dir, filename)\n",
    "    copyfile(source_path, destination_path)\n",
    "\n",
    "print(f\"Split the dataset into {len(train_files)} training images and {len(validate_files)} validation images.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "830746eb-f0a6-4c2f-a03e-32b3a6480407",
   "metadata": {},
   "outputs": [],
   "source": [
    "room_label = [(0,'LivingRoom'),\n",
    "            (1,'MasterRoom'),\n",
    "            (2,'Kitchen'),\n",
    "            (3,'Bathroom'),\n",
    "            (4,'DiningRoom'),\n",
    "            (5,'ChildRoom'),\n",
    "            (6,'StudyRoom'),\n",
    "            (7,'SecondRoom'),\n",
    "            (8,'GuestRoom'),\n",
    "            (9,'Balcony'),\n",
    "            (10,'Entrance'),\n",
    "            (11,'Storage'),\n",
    "            (12,'Wall-in'),\n",
    "            (13,'External'),\n",
    "            (14,'ExteriorWall'),\n",
    "            (15,'FrontDoor'),\n",
    "            (16,'InteriorWall'),\n",
    "            (17,'InteriorDoor')]\n",
    "\n",
    "category = [category for category in room_label if category[1] not in set(['External',\\\n",
    "            'ExteriorWall','FrontDoor','InteriorWall','InteriorDoor'])]\n",
    "\n",
    "num_category = len(category)\n",
    "\n",
    "pixel2length = 18/256\n",
    "\n",
    "def label2name(label=0):\n",
    "    if label < 0 or label > 17:\n",
    "        raise Exception(\"Invalid label!\", label)\n",
    "    else:\n",
    "        return room_label[label][1]\n",
    "\n",
    "def label2index(label=0):\n",
    "    if label < 0 or label > 17:\n",
    "        raise Exception(\"Invalid label!\", label)\n",
    "    else:\n",
    "        return label\n",
    "\n",
    "def index2label(index=0):\n",
    "    if index < 0 or index > 17:\n",
    "        raise Exception(\"Invalid index!\", index)\n",
    "    else:\n",
    "        return index\n",
    "\n",
    "def compute_centroid(mask):\n",
    "    sum_h = 0\n",
    "    sum_w = 0\n",
    "    count = 0\n",
    "    shape_array = mask.shape\n",
    "    for h in range(shape_array[0]):  \n",
    "        for w in range(shape_array[1]):\n",
    "            if mask[h, w] != 0:\n",
    "                sum_h += h\n",
    "                sum_w += w\n",
    "                count += 1\n",
    "    return (sum_h//count, sum_w//count)\n",
    "\n",
    "def log(file, msg='', is_print=True):\n",
    "    if is_print:\n",
    "        print(msg)\n",
    "    file.write(msg + '\\n')\n",
    "    file.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b729cf7-d78a-4ac0-a06e-4e41f1ac56cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from tqdm.notebook import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b4728378-79e5-4841-8f78-2cd97d7c9f84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*******************************************\n",
      "Number of dataset: 64630\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7813d333f78e4879b53af88ff640ea03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64630 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of dataset: 16158\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcd3b30897a242e8aea0c04903a9b777",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16158 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import shutil\n",
    "import pickle\n",
    "# import utils\n",
    "import os\n",
    "\n",
    "\n",
    "def write2pickle(train_dir, pkl_dir):\n",
    "    train_data_path = [os.path.join(train_dir, path) for path in os.listdir(train_dir)]\n",
    "    print(f'Number of dataset: {len(train_data_path)}')\n",
    "    for path in tqdm_notebook(train_data_path):\n",
    "        with Image.open(path) as temp:\n",
    "            image_array = np.asarray(temp, dtype=np.uint8)\n",
    "        boundary_mask = image_array[:,:,0]\n",
    "        category_mask = image_array[:,:,1]\n",
    "        index_mask = image_array[:,:,2]\n",
    "        inside_mask = image_array[:,:,3]\n",
    "        shape_array = image_array.shape\n",
    "        index_category = []\n",
    "        room_node = []\n",
    "\n",
    "        interiorWall_mask = np.zeros(category_mask.shape, dtype=np.uint8)\n",
    "        interiorWall_mask[category_mask == 16] = 1        \n",
    "        interiordoor_mask = np.zeros(category_mask.shape, dtype=np.uint8)\n",
    "        interiordoor_mask[category_mask == 17] = 1\n",
    "\n",
    "        for h in range(shape_array[0]):  \n",
    "            for w in range(shape_array[1]):\n",
    "                index = index_mask[h, w]\n",
    "                category = category_mask[h, w]\n",
    "                if index > 0 and category <= 12:\n",
    "                    if len(index_category):\n",
    "                        flag = True\n",
    "                        for i in index_category:\n",
    "                            if i[0] == index:\n",
    "                                flag = False\n",
    "                        if flag:\n",
    "                            index_category.append((index, category))\n",
    "                    else:\n",
    "                        index_category.append((index, category))\n",
    "\n",
    "        for (index, category) in index_category:\n",
    "            node = {}\n",
    "            node['category'] = int(category)\n",
    "            mask = np.zeros(index_mask.shape, dtype=np.uint8)\n",
    "            mask[index_mask == index] = 1\n",
    "            node['centroid'] = compute_centroid(mask)\n",
    "            room_node.append(node)\n",
    "        \n",
    "        pkl_path = path.replace(train_dir, pkl_dir)\n",
    "        pkl_path = pkl_path.replace('png', 'pkl')\n",
    "        pkl_file = open(pkl_path, 'wb')\n",
    "        pickle.dump([inside_mask, boundary_mask, interiorWall_mask, interiordoor_mask, room_node], \n",
    "            pkl_file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        pkl_file.close()\n",
    "\n",
    "if __name__=='__main__':\n",
    "    print(\"*******************************************\")\n",
    "    train_dataset_dir = r\"D:\\Projects and Training\\Projects\\floor plan\\dataset\\dataset\\train\"\n",
    "    val_dataset_dir = r\"D:\\Projects and Training\\Projects\\floor plan\\dataset\\dataset\\val\"\n",
    "    \n",
    "    train_pickle_dir = r\"D:\\Projects and Training\\Projects\\floor plan\\dataset\\dataset\\pickle\\train\"\n",
    "    val_pickle_dir = r\"D:\\Projects and Training\\Projects\\floor plan\\dataset\\dataset\\pickle\\val\"\n",
    "    if os.path.exists(train_pickle_dir):\n",
    "        shutil.rmtree(train_pickle_dir)\n",
    "    os.mkdir(train_pickle_dir)\n",
    "    if os.path.exists(val_pickle_dir):\n",
    "        shutil.rmtree(val_pickle_dir)\n",
    "    os.mkdir(val_pickle_dir)\n",
    "    write2pickle(train_dataset_dir, train_pickle_dir)\n",
    "    write2pickle(val_dataset_dir, val_pickle_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d64c1e-de24-43d8-8c5f-45a8648da2d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f28c387-772e-4ccc-bd62-9ec744b56c53",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
